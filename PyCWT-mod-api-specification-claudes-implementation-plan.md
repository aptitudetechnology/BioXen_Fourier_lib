# PyCWT-mod REST API Implementation Plan

**Document Version:** 1.0  
**Created:** 3 October 2025  
**Target:** FastAPI-based REST API for hardware-accelerated wavelet analysis  
**Scope:** Complete implementation from specification to production deployment

---

## ï¿½ï¸ Architectural Context

### **BioXen_Fourier_lib â†” PyCWT-mod Server Integration**

This implementation plan details a **REST API server** that extends BioXen_Fourier_lib's capabilities by providing remote access to pycwt-mod's hardware-accelerated wavelet analysis backends.

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    BioXen_Fourier_lib                           â”‚
â”‚  (Python Library for Multi-Signal Wavelet Analysis)            â”‚
â”‚                                                                 â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚  BioXen.wavelet_coherence(signal1, signal2)            â”‚  â”‚
â”‚  â”‚    â”œâ”€ Local computation (NumPy/SciPy)                  â”‚  â”‚
â”‚  â”‚    â””â”€ Remote computation (HTTP/WebSocket API)  â—„â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”¼â”€â”
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚ â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
                                                                    â”‚
                              REST API                              â”‚
                         (this implementation)                      â”‚
                                                                    â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚                    PyCWT-mod API Server                         â”‚â—„â”˜
â”‚  (FastAPI REST/WebSocket Interface)                             â”‚
â”‚                                                                  â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚  â”‚  Endpoints:                                                â”‚ â”‚
â”‚  â”‚    â€¢ POST /api/v1/wavelet/wct                             â”‚ â”‚
â”‚  â”‚    â€¢ WS   /ws/stream/wct                                  â”‚ â”‚
â”‚  â”‚    â€¢ GET  /api/v1/backends/                               â”‚ â”‚
â”‚  â”‚    â€¢ POST /api/v1/benchmark                               â”‚ â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â”‚                            â–¼                                     â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚  â”‚  Backend Selector (pycwt-mod integration)                 â”‚ â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â”‚       â”‚              â”‚                â”‚                          â”‚
â”‚       â–¼              â–¼                â–¼                          â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                  â”‚
â”‚  â”‚  CPU   â”‚   â”‚  Joblib  â”‚   â”‚  FPGA/ELM11  â”‚                  â”‚
â”‚  â”‚ Backendâ”‚   â”‚  Parallelâ”‚   â”‚  Tang Nano9K â”‚                  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                  â”‚
â”‚                                      â”‚                           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                       â–¼
                            â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                            â”‚  Hardware Device â”‚
                            â”‚  (Serial/USB)    â”‚
                            â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### **Key Integration Points:**

1. **BioXen Client Library Extension** (Python):
   ```python
   # BioXen_Fourier_lib/bioxen/wavelet/remote.py
   class RemoteWaveletBackend:
       def __init__(self, api_url="http://localhost:8000"):
           self.client = PyCWTClient(api_url)
       
       def wavelet_coherence(self, signal1, signal2, **kwargs):
           return self.client.wavelet_coherence(signal1, signal2, **kwargs)
   ```

2. **Backend Selection Strategy**:
   ```python
   # User can choose local or remote computation
   from bioxen.wavelet import WaveletAnalyzer
   
   # Local computation (NumPy/SciPy)
   analyzer = WaveletAnalyzer(backend='local')
   
   # Remote computation with FPGA acceleration
   analyzer = WaveletAnalyzer(
       backend='remote',
       api_url='http://server:8000',
       remote_backend='elm11'  # Tang Nano 9K FPGA
   )
   ```

3. **Use Cases for Remote API**:
   - âœ… **Hardware acceleration** without local FPGA installation
   - âœ… **Batch processing** of large datasets on server infrastructure
   - âœ… **Real-time streaming** for BCI/adaptive stimulation experiments
   - âœ… **Cross-platform compatibility** (web apps, R, MATLAB integration)
   - âœ… **Shared resources** in lab environments (one FPGA server, many clients)

---

## ðŸŽ¯ Executive Summary

This implementation plan transforms the PyCWT-mod API specification into a production-ready REST API server that **extends BioXen_Fourier_lib** by providing remote access to hardware-accelerated wavelet coherence analysis (CPU, FPGA, embedded systems). The API enables BioXen users to leverage pycwt-mod's hardware backends without local installation, while also supporting direct HTTP/WebSocket access from other languages and platforms.

**Timeline:** 6-8 weeks  
**Complexity:** Medium-High (backend integration, hardware detection, async processing)  
**Risk Level:** Medium (hardware dependencies, performance validation)  
**Architecture:** Client-Server (BioXen clients â†’ REST API â†’ pycwt-mod backends â†’ hardware)

---

## ðŸ“‹ Implementation Phases

### **Phase 1: Core API Infrastructure (Week 1-2)**

#### **1.1 Project Structure Setup**

```
pycwt-mod/
â”œâ”€â”€ server/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ main.py                    # FastAPI app entry point
â”‚   â”œâ”€â”€ config.py                  # Environment config
â”‚   â”œâ”€â”€ dependencies.py            # Dependency injection
â”‚   â”‚
â”‚   â”œâ”€â”€ api/
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ v1/
â”‚   â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”‚   â”œâ”€â”€ routes/
â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ backends.py    # Backend management
â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ wavelet.py     # Wavelet endpoints
â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ jobs.py        # Batch processing
â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ hardware.py    # Hardware detection
â”‚   â”‚   â”‚   â”‚   â””â”€â”€ benchmark.py   # Performance testing
â”‚   â”‚   â”‚   â”‚
â”‚   â”‚   â”‚   â””â”€â”€ models/
â”‚   â”‚   â”‚       â”œâ”€â”€ __init__.py
â”‚   â”‚   â”‚       â”œâ”€â”€ requests.py    # Pydantic request models
â”‚   â”‚   â”‚       â”œâ”€â”€ responses.py   # Pydantic response models
â”‚   â”‚   â”‚       â””â”€â”€ schemas.py     # Shared schemas
â”‚   â”‚   â”‚
â”‚   â”‚   â””â”€â”€ websockets/
â”‚   â”‚       â”œâ”€â”€ __init__.py
â”‚   â”‚       â””â”€â”€ streaming.py       # Real-time streaming
â”‚   â”‚
â”‚   â”œâ”€â”€ services/
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ wavelet_service.py     # Wavelet computation logic
â”‚   â”‚   â”œâ”€â”€ backend_service.py     # Backend management
â”‚   â”‚   â”œâ”€â”€ job_service.py         # Job queue management
â”‚   â”‚   â””â”€â”€ hardware_service.py    # Hardware detection
â”‚   â”‚
â”‚   â”œâ”€â”€ core/
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ security.py            # Auth (future)
â”‚   â”‚   â”œâ”€â”€ logging.py             # Structured logging
â”‚   â”‚   â””â”€â”€ errors.py              # Error handlers
â”‚   â”‚
â”‚   â””â”€â”€ tests/
â”‚       â”œâ”€â”€ __init__.py
â”‚       â”œâ”€â”€ conftest.py            # Pytest fixtures
â”‚       â”œâ”€â”€ test_backends.py
â”‚       â”œâ”€â”€ test_wavelet.py
â”‚       â”œâ”€â”€ test_jobs.py
â”‚       â””â”€â”€ test_integration.py
â”‚
â”œâ”€â”€ .env.example                   # Example environment config
â”œâ”€â”€ docker-compose.yml             # Docker deployment
â”œâ”€â”€ Dockerfile                     # API container
â”œâ”€â”€ requirements-server.txt        # Server dependencies
â””â”€â”€ pyproject.toml                # Updated with server extras
```

#### **1.2 Dependencies Installation**

**Core Dependencies (requirements-server.txt):**
```txt
# FastAPI Framework
fastapi>=0.104.0
uvicorn[standard]>=0.24.0
pydantic>=2.4.0
pydantic-settings>=2.0.3

# Async Support
aiofiles>=23.2.1
python-multipart>=0.0.6

# Job Queue
celery>=5.3.4
redis>=5.0.0

# Monitoring
prometheus-client>=0.18.0
python-json-logger>=2.0.7

# CORS
python-cors>=1.0.0

# PyCWT-mod integration
pycwt-mod>=1.0.0
numpy>=1.24.0
scipy>=1.11.0

# Hardware backends
pyserial>=3.5
joblib>=1.3.0
```

#### **1.3 FastAPI Application Scaffold**

**server/main.py:**
```python
from fastapi import FastAPI
from fastapi.middleware.cors import CORSMiddleware
from fastapi.middleware.gzip import GZipMiddleware
from contextlib import asynccontextmanager
import logging

from server.config import settings
from server.api.v1.routes import backends, wavelet, jobs, hardware, benchmark
from server.api.websockets import streaming
from server.core.logging import setup_logging
from server.core.errors import add_exception_handlers

# Setup logging
setup_logging()
logger = logging.getLogger(__name__)


@asynccontextmanager
async def lifespan(app: FastAPI):
    """Startup and shutdown lifecycle."""
    logger.info("Starting PyCWT-mod API server...")
    
    # Initialize backends on startup
    from server.services.backend_service import BackendService
    backend_service = BackendService()
    await backend_service.initialize()
    
    yield
    
    # Cleanup on shutdown
    logger.info("Shutting down PyCWT-mod API server...")
    await backend_service.cleanup()


# Create FastAPI app
app = FastAPI(
    title="PyCWT-mod REST API",
    description="Hardware-accelerated wavelet analysis API",
    version="0.1.0-alpha",
    docs_url="/docs",
    redoc_url="/redoc",
    lifespan=lifespan
)

# CORS middleware
app.add_middleware(
    CORSMiddleware,
    allow_origins=settings.CORS_ORIGINS,
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# Gzip compression
app.add_middleware(GZipMiddleware, minimum_size=1000)

# Exception handlers
add_exception_handlers(app)

# Root endpoint
@app.get("/")
async def root():
    return {
        "name": "PyCWT-mod REST API",
        "version": "0.1.0-alpha",
        "docs": "/docs",
        "health": "/health"
    }

# Health check
@app.get("/health")
async def health_check():
    return {
        "status": "healthy",
        "api_version": "0.1.0-alpha"
    }

# Include routers
app.include_router(backends.router, prefix="/api/v1", tags=["backends"])
app.include_router(wavelet.router, prefix="/api/v1", tags=["wavelet"])
app.include_router(jobs.router, prefix="/api/v1", tags=["jobs"])
app.include_router(hardware.router, prefix="/api/v1", tags=["hardware"])
app.include_router(benchmark.router, prefix="/api/v1", tags=["benchmark"])

# WebSocket endpoint
app.include_router(streaming.router, prefix="/ws", tags=["streaming"])


if __name__ == "__main__":
    import uvicorn
    uvicorn.run(
        "server.main:app",
        host=settings.HOST,
        port=settings.PORT,
        reload=settings.DEBUG,
        log_level="info"
    )
```

#### **1.4 Configuration Management**

**server/config.py:**
```python
from pydantic_settings import BaseSettings
from typing import List

class Settings(BaseSettings):
    """Application settings."""
    
    # Server
    HOST: str = "0.0.0.0"
    PORT: int = 8000
    DEBUG: bool = False
    
    # CORS
    CORS_ORIGINS: List[str] = ["http://localhost:3000", "http://localhost:8080"]
    
    # Backends
    DEFAULT_BACKEND: str = "sequential"
    AVAILABLE_BACKENDS: str = "sequential,joblib,elm11"
    
    # Job Management
    MAX_CONCURRENT_JOBS: int = 10
    JOB_TIMEOUT_SECONDS: int = 3600
    REDIS_URL: str = "redis://localhost:6379/0"
    
    # Hardware Detection
    HARDWARE_DETECTION_TIMEOUT: int = 5
    ENABLE_FPGA: bool = True
    
    # Logging
    LOG_LEVEL: str = "INFO"
    LOG_FORMAT: str = "json"
    
    class Config:
        env_file = ".env"
        case_sensitive = True


settings = Settings()
```

---

### **Phase 2: Backend Management Endpoints (Week 2)**

#### **2.1 Backend Service**

**server/services/backend_service.py:**
```python
from typing import Dict, List, Optional
import logging
from pycwt_mod.backends import list_backends, get_backend

logger = logging.getLogger(__name__)


class BackendService:
    """Service for managing computation backends."""
    
    def __init__(self):
        self._backends_cache = {}
        self._initialized = False
    
    async def initialize(self):
        """Initialize and cache backend information."""
        logger.info("Initializing backends...")
        
        backend_names = list_backends()
        for name in backend_names:
            try:
                backend = get_backend(name)
                self._backends_cache[name] = {
                    "name": name,
                    "available": backend.is_available(),
                    "description": backend.__doc__ or "No description",
                    "type": backend.__class__.__name__
                }
                logger.info(f"Backend '{name}': available={backend.is_available()}")
            except Exception as e:
                logger.warning(f"Failed to initialize backend '{name}': {e}")
                self._backends_cache[name] = {
                    "name": name,
                    "available": False,
                    "description": "Failed to load",
                    "type": "Unknown",
                    "error": str(e)
                }
        
        self._initialized = True
    
    async def list_all_backends(self) -> List[Dict]:
        """List all registered backends."""
        if not self._initialized:
            await self.initialize()
        
        return list(self._backends_cache.values())
    
    async def get_backend_info(self, name: str) -> Optional[Dict]:
        """Get information about a specific backend."""
        if not self._initialized:
            await self.initialize()
        
        return self._backends_cache.get(name)
    
    async def cleanup(self):
        """Cleanup resources."""
        logger.info("Cleaning up backends...")
        self._backends_cache.clear()
```

#### **2.2 Backend Routes**

**server/api/v1/routes/backends.py:**
```python
from fastapi import APIRouter, HTTPException, Depends
from typing import List
from server.api.v1.models.responses import BackendListResponse, BackendInfoResponse
from server.services.backend_service import BackendService

router = APIRouter()


def get_backend_service():
    """Dependency injection for backend service."""
    return BackendService()


@router.get("/backends/", response_model=BackendListResponse)
async def list_backends(
    backend_service: BackendService = Depends(get_backend_service)
):
    """
    List all available computation backends.
    
    Returns information about each registered backend including:
    - Name and type
    - Hardware/software availability
    - Description and capabilities
    """
    backends = await backend_service.list_all_backends()
    return {"backends": backends}


@router.get("/backends/{backend_name}", response_model=BackendInfoResponse)
async def get_backend(
    backend_name: str,
    backend_service: BackendService = Depends(get_backend_service)
):
    """
    Get detailed information about a specific backend.
    
    Parameters:
    - backend_name: Backend identifier (e.g., 'sequential', 'joblib', 'elm11')
    """
    backend_info = await backend_service.get_backend_info(backend_name)
    
    if backend_info is None:
        raise HTTPException(
            status_code=404,
            detail=f"Backend '{backend_name}' not found"
        )
    
    return backend_info
```

#### **2.3 Response Models**

**server/api/v1/models/responses.py:**
```python
from pydantic import BaseModel, Field
from typing import List, Optional


class BackendInfo(BaseModel):
    """Backend information model."""
    name: str = Field(..., description="Backend identifier")
    available: bool = Field(..., description="Hardware/software availability")
    description: str = Field(..., description="Backend description")
    type: str = Field(..., description="Backend class name")
    error: Optional[str] = Field(None, description="Error message if unavailable")


class BackendListResponse(BaseModel):
    """Response model for listing backends."""
    backends: List[BackendInfo] = Field(..., description="List of available backends")


class BackendInfoResponse(BackendInfo):
    """Response model for single backend information."""
    pass
```

---

### **Phase 3: Wavelet Analysis Endpoints (Week 3)**

#### **3.1 Wavelet Service**

**server/services/wavelet_service.py:**
```python
import numpy as np
import time
import logging
from typing import Tuple, Optional
import pycwt_mod as pycwt

logger = logging.getLogger(__name__)


class WaveletService:
    """Service for wavelet transform computations."""
    
    async def compute_cwt(
        self,
        data: np.ndarray,
        dt: float,
        dj: float = 0.25,
        s0: float = -1,
        J: int = -1,
        mother: str = "morlet",
        param: float = -1
    ) -> dict:
        """
        Compute continuous wavelet transform.
        
        Returns:
        - wave: Wavelet transform coefficients
        - scales: Wavelet scales
        - freqs: Corresponding frequencies
        - coi: Cone of influence
        - fft: FFT of input data
        - fftfreqs: FFT frequencies
        """
        logger.info(f"Computing CWT for {len(data)} points")
        
        start_time = time.time()
        
        try:
            wave, scales, freqs, coi, fft, fftfreqs = pycwt.cwt(
                data, dt, dj=dj, s0=s0, J=J, wavelet=mother, param=param
            )
            
            computation_time = time.time() - start_time
            logger.info(f"CWT completed in {computation_time:.3f}s")
            
            return {
                "wave": wave.tolist(),  # Complex values
                "scales": scales.tolist(),
                "freqs": freqs.tolist(),
                "coi": coi.tolist(),
                "fft": fft.tolist(),
                "fftfreqs": fftfreqs.tolist(),
                "computation_time": computation_time
            }
        
        except Exception as e:
            logger.error(f"CWT computation failed: {e}")
            raise
    
    async def compute_wct(
        self,
        signal1: np.ndarray,
        signal2: np.ndarray,
        dt: float,
        dj: float = 0.25,
        s0: float = -1,
        J: int = -1,
        significance_level: float = 0.95,
        mc_count: int = 300,
        backend: str = "sequential"
    ) -> dict:
        """
        Compute wavelet coherence between two signals.
        
        Returns:
        - WCT: Wavelet coherence transform
        - aWCT: Phase angles
        - coi: Cone of influence
        - freqs: Frequencies
        - signif: Significance levels
        - backend_used: Actual backend used
        - computation_time: Time taken
        """
        logger.info(f"Computing WCT with backend='{backend}', mc_count={mc_count}")
        
        start_time = time.time()
        
        try:
            WCT, aWCT, coi, freqs, sig95 = pycwt.wct(
                signal1, signal2,
                dt=dt, dj=dj, s0=s0, J=J,
                sig=True,
                significance_level=significance_level,
                mc_count=mc_count,
                backend=backend
            )
            
            computation_time = time.time() - start_time
            logger.info(f"WCT completed in {computation_time:.3f}s")
            
            return {
                "WCT": WCT.tolist(),
                "aWCT": aWCT.tolist(),
                "coi": coi.tolist(),
                "freqs": freqs.tolist(),
                "signif": sig95.tolist(),
                "backend_used": backend,
                "computation_time": computation_time
            }
        
        except Exception as e:
            logger.error(f"WCT computation failed: {e}")
            raise
    
    async def compute_xwt(
        self,
        signal1: np.ndarray,
        signal2: np.ndarray,
        dt: float,
        dj: float = 0.25,
        s0: float = -1,
        J: int = -1
    ) -> dict:
        """
        Compute cross-wavelet transform.
        
        Returns:
        - xwt: Cross-wavelet transform
        - phase: Phase angles
        - coi: Cone of influence
        - freqs: Frequencies
        """
        logger.info(f"Computing XWT for signals of length {len(signal1)}")
        
        start_time = time.time()
        
        try:
            xwt, phase, coi, freqs = pycwt.xwt(
                signal1, signal2,
                dt=dt, dj=dj, s0=s0, J=J
            )
            
            computation_time = time.time() - start_time
            logger.info(f"XWT completed in {computation_time:.3f}s")
            
            return {
                "xwt": xwt.tolist(),
                "phase": phase.tolist(),
                "coi": coi.tolist(),
                "freqs": freqs.tolist(),
                "computation_time": computation_time
            }
        
        except Exception as e:
            logger.error(f"XWT computation failed: {e}")
            raise
```

#### **3.2 Wavelet Routes**

**server/api/v1/routes/wavelet.py:**
```python
from fastapi import APIRouter, HTTPException, Depends
from server.api.v1.models.requests import CWTRequest, WCTRequest, XWTRequest
from server.api.v1.models.responses import CWTResponse, WCTResponse, XWTResponse
from server.services.wavelet_service import WaveletService
import numpy as np

router = APIRouter()


def get_wavelet_service():
    """Dependency injection for wavelet service."""
    return WaveletService()


@router.post("/wavelet/cwt", response_model=CWTResponse)
async def continuous_wavelet_transform(
    request: CWTRequest,
    wavelet_service: WaveletService = Depends(get_wavelet_service)
):
    """
    Perform continuous wavelet transform on time series data.
    
    Parameters:
    - data: Time series array
    - dt: Time step
    - dj: Scale resolution (default: 0.25)
    - s0: Smallest scale (auto if -1)
    - J: Number of scales (auto if -1)
    - mother: Wavelet type (default: 'morlet')
    - param: Wavelet parameter (default: -1)
    """
    try:
        data = np.array(request.data)
        
        result = await wavelet_service.compute_cwt(
            data=data,
            dt=request.dt,
            dj=request.dj,
            s0=request.s0,
            J=request.J,
            mother=request.mother,
            param=request.param
        )
        
        return result
    
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))


@router.post("/wavelet/wct", response_model=WCTResponse)
async def wavelet_coherence(
    request: WCTRequest,
    wavelet_service: WaveletService = Depends(get_wavelet_service)
):
    """
    Calculate wavelet coherence between two time series.
    
    Parameters:
    - signal1, signal2: Time series arrays
    - dt: Time step
    - dj: Scale resolution (default: 0.25)
    - significance_level: Significance level (default: 0.95)
    - mc_count: Monte Carlo simulations (default: 300)
    - backend: Computation backend (sequential/joblib/elm11)
    """
    try:
        signal1 = np.array(request.signal1)
        signal2 = np.array(request.signal2)
        
        if len(signal1) != len(signal2):
            raise HTTPException(
                status_code=400,
                detail="signal1 and signal2 must have the same length"
            )
        
        result = await wavelet_service.compute_wct(
            signal1=signal1,
            signal2=signal2,
            dt=request.dt,
            dj=request.dj,
            s0=request.s0,
            J=request.J,
            significance_level=request.significance_level,
            mc_count=request.mc_count,
            backend=request.backend
        )
        
        return result
    
    except HTTPException:
        raise
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))


@router.post("/wavelet/xwt", response_model=XWTResponse)
async def cross_wavelet_transform(
    request: XWTRequest,
    wavelet_service: WaveletService = Depends(get_wavelet_service)
):
    """
    Calculate cross-wavelet transform between two time series.
    
    Parameters:
    - signal1, signal2: Time series arrays
    - dt: Time step
    - dj: Scale resolution (default: 0.25)
    """
    try:
        signal1 = np.array(request.signal1)
        signal2 = np.array(request.signal2)
        
        if len(signal1) != len(signal2):
            raise HTTPException(
                status_code=400,
                detail="signal1 and signal2 must have the same length"
            )
        
        result = await wavelet_service.compute_xwt(
            signal1=signal1,
            signal2=signal2,
            dt=request.dt,
            dj=request.dj,
            s0=request.s0,
            J=request.J
        )
        
        return result
    
    except HTTPException:
        raise
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))
```

---

### **Phase 4: Real-Time Streaming (Week 4)**

#### **4.1 WebSocket Streaming Endpoint**

**server/api/websockets/streaming.py:**
```python
from fastapi import APIRouter, WebSocket, WebSocketDisconnect
from typing import Dict, Optional
import json
import logging
import numpy as np
from server.services.wavelet_service import WaveletService

logger = logging.getLogger(__name__)
router = APIRouter()


class StreamingManager:
    """Manage real-time wavelet analysis streams."""
    
    def __init__(self):
        self.active_connections: Dict[str, WebSocket] = {}
        self.wavelet_service = WaveletService()
    
    async def connect(self, stream_id: str, websocket: WebSocket):
        """Accept new streaming connection."""
        await websocket.accept()
        self.active_connections[stream_id] = websocket
        logger.info(f"Stream {stream_id} connected")
    
    def disconnect(self, stream_id: str):
        """Remove streaming connection."""
        if stream_id in self.active_connections:
            del self.active_connections[stream_id]
            logger.info(f"Stream {stream_id} disconnected")
    
    async def process_chunk(
        self,
        websocket: WebSocket,
        signal1_chunk: np.ndarray,
        signal2_chunk: np.ndarray,
        dt: float,
        backend: str = "sequential"
    ):
        """Process signal chunk and send results."""
        try:
            # Compute wavelet coherence for chunk
            result = await self.wavelet_service.compute_wct(
                signal1=signal1_chunk,
                signal2=signal2_chunk,
                dt=dt,
                backend=backend,
                mc_count=50  # Reduced for real-time
            )
            
            # Send result back to client
            await websocket.send_json({
                "type": "result",
                "data": result
            })
        
        except Exception as e:
            logger.error(f"Error processing chunk: {e}")
            await websocket.send_json({
                "type": "error",
                "message": str(e)
            })


# Global streaming manager
streaming_manager = StreamingManager()


@router.websocket("/stream/wct")
async def websocket_wavelet_coherence(websocket: WebSocket):
    """
    Real-time wavelet coherence streaming endpoint.
    
    This WebSocket endpoint enables continuous processing of signal streams
    for applications requiring low-latency analysis (BCI, adaptive stimulation).
    
    Protocol:
    1. Client sends configuration:
       {"type": "config", "dt": 0.001, "backend": "elm11"}
    
    2. Client sends signal chunks:
       {"type": "data", "signal1": [...], "signal2": [...]}
    
    3. Server responds with results:
       {"type": "result", "data": {...}}
    """
    stream_id = f"stream_{id(websocket)}"
    
    await streaming_manager.connect(stream_id, websocket)
    
    config = {"dt": 0.1, "backend": "sequential"}
    
    try:
        while True:
            # Receive message from client
            message = await websocket.receive_json()
            
            if message["type"] == "config":
                # Update configuration
                config.update(message)
                await websocket.send_json({
                    "type": "ack",
                    "message": "Configuration updated"
                })
            
            elif message["type"] == "data":
                # Process signal chunk
                signal1 = np.array(message["signal1"])
                signal2 = np.array(message["signal2"])
                
                await streaming_manager.process_chunk(
                    websocket=websocket,
                    signal1_chunk=signal1,
                    signal2_chunk=signal2,
                    dt=config["dt"],
                    backend=config.get("backend", "sequential")
                )
            
            elif message["type"] == "ping":
                # Keepalive
                await websocket.send_json({"type": "pong"})
    
    except WebSocketDisconnect:
        streaming_manager.disconnect(stream_id)
    
    except Exception as e:
        logger.error(f"WebSocket error: {e}")
        streaming_manager.disconnect(stream_id)
```

---

### **Phase 5: Hardware Detection & Benchmarking (Week 5)**

#### **5.1 Hardware Detection Service**

**server/services/hardware_service.py:**
```python
import logging
import subprocess
from typing import Dict, List
from server.config import settings

logger = logging.getLogger(__name__)


class HardwareService:
    """Service for detecting and managing hardware accelerators."""
    
    async def detect_all_hardware(self) -> Dict:
        """
        Detect all available hardware accelerators.
        
        Returns information about:
        - FPGA devices (Tang Nano 9K)
        - Embedded systems (ELM11)
        - GPU availability
        - CPU capabilities
        """
        logger.info("Running hardware detection...")
        
        results = {
            "fpga": await self._detect_fpga(),
            "embedded": await self._detect_embedded(),
            "gpu": await self._detect_gpu(),
            "cpu": await self._detect_cpu()
        }
        
        return results
    
    async def _detect_fpga(self) -> Dict:
        """Detect FPGA devices (Tang Nano 9K)."""
        try:
            from pycwt_mod.backends import get_backend
            backend = get_backend("elm11")
            
            if backend.is_available():
                return {
                    "available": True,
                    "device": "Tang Nano 9K",
                    "interface": "Serial/USB",
                    "status": "Connected"
                }
            else:
                return {
                    "available": False,
                    "device": "Tang Nano 9K",
                    "status": "Not detected"
                }
        
        except Exception as e:
            logger.warning(f"FPGA detection failed: {e}")
            return {
                "available": False,
                "error": str(e)
            }
    
    async def _detect_embedded(self) -> Dict:
        """Detect embedded systems (ELM11)."""
        try:
            import serial.tools.list_ports
            
            # Look for ELM11-compatible devices
            ports = serial.tools.list_ports.comports()
            embedded_devices = []
            
            for port in ports:
                if "USB" in port.description or "Serial" in port.description:
                    embedded_devices.append({
                        "port": port.device,
                        "description": port.description
                    })
            
            return {
                "available": len(embedded_devices) > 0,
                "devices": embedded_devices
            }
        
        except Exception as e:
            logger.warning(f"Embedded detection failed: {e}")
            return {"available": False, "error": str(e)}
    
    async def _detect_gpu(self) -> Dict:
        """Detect GPU availability."""
        try:
            # Check for CUDA
            result = subprocess.run(
                ["nvidia-smi", "--query-gpu=name,memory.total", "--format=csv,noheader"],
                capture_output=True,
                text=True,
                timeout=5
            )
            
            if result.returncode == 0:
                gpu_info = result.stdout.strip().split('\n')
                return {
                    "available": True,
                    "devices": [line.split(',') for line in gpu_info],
                    "type": "NVIDIA CUDA"
                }
        
        except (FileNotFoundError, subprocess.TimeoutExpired):
            pass
        
        return {"available": False, "type": "None"}
    
    async def _detect_cpu(self) -> Dict:
        """Get CPU capabilities."""
        import multiprocessing
        
        return {
            "cores": multiprocessing.cpu_count(),
            "available": True
        }
```

#### **5.2 Hardware Routes**

**server/api/v1/routes/hardware.py:**
```python
from fastapi import APIRouter, Depends
from server.services.hardware_service import HardwareService
from server.api.v1.models.responses import HardwareDetectionResponse

router = APIRouter()


def get_hardware_service():
    """Dependency injection for hardware service."""
    return HardwareService()


@router.get("/hardware/detect", response_model=HardwareDetectionResponse)
async def detect_hardware(
    hardware_service: HardwareService = Depends(get_hardware_service)
):
    """
    Detect all available hardware accelerators.
    
    This endpoint scans for:
    - FPGA devices (Tang Nano 9K via serial)
    - Embedded systems (ELM11 microcontrollers)
    - GPU availability (NVIDIA CUDA)
    - CPU core count
    
    Useful for automated backend selection and troubleshooting.
    """
    results = await hardware_service.detect_all_hardware()
    return results
```

#### **5.3 Benchmark Service**

**server/api/v1/routes/benchmark.py:**
```python
from fastapi import APIRouter, Depends, BackgroundTasks
from server.api.v1.models.requests import BenchmarkRequest
from server.api.v1.models.responses import BenchmarkResponse
from server.services.wavelet_service import WaveletService
import numpy as np
import time
from typing import Dict

router = APIRouter()


@router.post("/benchmark", response_model=BenchmarkResponse)
async def run_benchmark(
    request: BenchmarkRequest,
    background_tasks: BackgroundTasks
):
    """
    Run performance benchmarks across all available backends.
    
    This automates the MVP research questions by testing:
    - pycwt performance characteristics (Question 1)
    - PyWavelets vs pycwt comparison (Question 2)
    - FPGA acceleration benefits (FPGA research)
    
    Parameters:
    - signal_length: Number of timepoints to test
    - mc_count: Monte Carlo iterations
    - backends: List of backends to test (default: all available)
    """
    wavelet_service = WaveletService()
    
    # Generate test signals
    t = np.linspace(0, 10, request.signal_length)
    signal1 = np.sin(2 * np.pi * t)
    signal2 = np.cos(2 * np.pi * t)
    
    results = {}
    
    for backend in request.backends:
        try:
            start = time.time()
            
            result = await wavelet_service.compute_wct(
                signal1=signal1,
                signal2=signal2,
                dt=0.1,
                mc_count=request.mc_count,
                backend=backend
            )
            
            elapsed = time.time() - start
            
            results[backend] = {
                "status": "completed",
                "computation_time": elapsed,
                "speedup": 1.0  # Computed relative to sequential
            }
        
        except Exception as e:
            results[backend] = {
                "status": "failed",
                "error": str(e)
            }
    
    # Calculate speedups relative to sequential
    if "sequential" in results and results["sequential"]["status"] == "completed":
        baseline = results["sequential"]["computation_time"]
        for backend, data in results.items():
            if data["status"] == "completed":
                data["speedup"] = baseline / data["computation_time"]
    
    return {
        "signal_length": request.signal_length,
        "mc_count": request.mc_count,
        "results": results
    }
```

---

### **Phase 6: Job Queue & Batch Processing (Week 6)**

#### **6.1 Job Service with Redis/Celery**

**server/services/job_service.py:**
```python
from celery import Celery
from typing import Dict, List
import uuid
import logging
from server.config import settings

logger = logging.getLogger(__name__)

# Celery app
celery_app = Celery(
    "pycwt_jobs",
    broker=settings.REDIS_URL,
    backend=settings.REDIS_URL
)


class JobService:
    """Service for managing batch job processing."""
    
    def __init__(self):
        self.celery = celery_app
    
    async def submit_batch_job(self, tasks: List[Dict]) -> Dict:
        """
        Submit a batch of wavelet analysis tasks.
        
        Returns job ID for tracking.
        """
        job_id = str(uuid.uuid4())
        
        logger.info(f"Submitting batch job {job_id} with {len(tasks)} tasks")
        
        # Queue tasks with Celery
        for i, task in enumerate(tasks):
            process_wavelet_task.apply_async(
                args=[task],
                task_id=f"{job_id}_{i}"
            )
        
        return {
            "job_id": job_id,
            "status": "queued",
            "tasks_count": len(tasks)
        }
    
    async def get_job_status(self, job_id: str) -> Dict:
        """Get status of a batch job."""
        # Query Celery for task status
        # Implementation depends on job tracking strategy
        
        return {
            "job_id": job_id,
            "status": "completed",  # queued, processing, completed, failed
            "progress": 100
        }


@celery_app.task
def process_wavelet_task(task: Dict):
    """Celery task for processing individual wavelet analysis."""
    import numpy as np
    from server.services.wavelet_service import WaveletService
    
    service = WaveletService()
    
    if task["type"] == "wct":
        signal1 = np.array(task["signal1"])
        signal2 = np.array(task["signal2"])
        # Use synchronous version or asyncio.run()
        # result = service.compute_wct(...)
        pass
    
    # Process other task types...
```

---

### **Phase 7: Testing & Documentation (Week 7)**

#### **7.1 Comprehensive Test Suite**

**server/tests/test_wavelet.py:**
```python
import pytest
from fastapi.testclient import TestClient
from server.main import app
import numpy as np

client = TestClient(app)


def test_cwt_basic():
    """Test basic CWT computation."""
    data = np.sin(np.linspace(0, 10, 100)).tolist()
    
    response = client.post("/api/v1/wavelet/cwt", json={
        "data": data,
        "dt": 0.1
    })
    
    assert response.status_code == 200
    result = response.json()
    assert "wave" in result
    assert "computation_time" in result


def test_wct_with_backends():
    """Test WCT with different backends."""
    signal1 = np.sin(np.linspace(0, 10, 50)).tolist()
    signal2 = np.cos(np.linspace(0, 10, 50)).tolist()
    
    backends = ["sequential", "joblib"]
    
    for backend in backends:
        response = client.post("/api/v1/wavelet/wct", json={
            "signal1": signal1,
            "signal2": signal2,
            "dt": 0.1,
            "backend": backend
        })
        
        assert response.status_code == 200
        result = response.json()
        assert result["backend_used"] == backend


def test_invalid_input():
    """Test validation of invalid inputs."""
    response = client.post("/api/v1/wavelet/wct", json={
        "signal1": [1, 2, 3],
        "signal2": [1, 2],  # Different length
        "dt": 0.1
    })
    
    assert response.status_code == 400
```

#### **7.2 Integration Tests**

**server/tests/test_integration.py:**
```python
import pytest
from server.main import app
from fastapi.testclient import TestClient

client = TestClient(app)


def test_full_workflow():
    """Test complete workflow: backend detection â†’ WCT computation."""
    # 1. Check health
    response = client.get("/health")
    assert response.status_code == 200
    
    # 2. List backends
    response = client.get("/api/v1/backends/")
    assert response.status_code == 200
    backends = response.json()["backends"]
    
    # 3. Detect hardware
    response = client.get("/api/v1/hardware/detect")
    assert response.status_code == 200
    
    # 4. Run WCT
    import numpy as np
    signal1 = np.sin(np.linspace(0, 10, 100)).tolist()
    signal2 = np.cos(np.linspace(0, 10, 100)).tolist()
    
    response = client.post("/api/v1/wavelet/wct", json={
        "signal1": signal1,
        "signal2": signal2,
        "dt": 0.1,
        "backend": "sequential"
    })
    
    assert response.status_code == 200
    assert "WCT" in response.json()
```

---

### **Phase 8: Deployment & Production (Week 8)**

#### **8.1 Docker Configuration**

**Dockerfile:**
```dockerfile
FROM python:3.11-slim

WORKDIR /app

# Install system dependencies
RUN apt-get update && apt-get install -y \
    gcc \
    g++ \
    && rm -rf /var/lib/apt/lists/*

# Copy requirements
COPY requirements-server.txt .
RUN pip install --no-cache-dir -r requirements-server.txt

# Copy application
COPY . .

# Expose port
EXPOSE 8000

# Run server
CMD ["uvicorn", "server.main:app", "--host", "0.0.0.0", "--port", "8000"]
```

**docker-compose.yml:**
```yaml
version: '3.8'

services:
  api:
    build: .
    ports:
      - "8000:8000"
    environment:
      - REDIS_URL=redis://redis:6379/0
    depends_on:
      - redis
    volumes:
      - ./server:/app/server
    devices:
      - /dev/ttyUSB0:/dev/ttyUSB0  # FPGA device
    privileged: true
  
  redis:
    image: redis:7-alpine
    ports:
      - "6379:6379"
  
  celery_worker:
    build: .
    command: celery -A server.services.job_service:celery_app worker --loglevel=info
    environment:
      - REDIS_URL=redis://redis:6379/0
    depends_on:
      - redis
    volumes:
      - ./server:/app/server
```

---

## ðŸŽ¯ Success Metrics

### **Performance Targets:**
- âœ… API response time <100ms for backend listing
- âœ… WCT computation time matches direct pycwt usage (minimal overhead)
- âœ… WebSocket latency <50ms for streaming
- âœ… Support 10+ concurrent requests

### **Reliability Targets:**
- âœ… 99% uptime for API server
- âœ… Graceful fallback when hardware unavailable
- âœ… Comprehensive error messages
- âœ… 80%+ test coverage

### **Documentation Targets:**
- âœ… Complete OpenAPI/Swagger documentation
- âœ… Client library examples (Python, JS, R, MATLAB)
- âœ… Deployment guide (Docker, bare metal)
- âœ… Troubleshooting guide

---

## ðŸ”— Integration with BioXen_Fourier_lib & Research Documents

### **BioXen Client Library Integration:**

**Step 1: Add Remote Backend to BioXen_Fourier_lib**

Create `BioXen_Fourier_lib/bioxen/wavelet/remote_backend.py`:
```python
"""
Remote wavelet backend for BioXen_Fourier_lib.

Connects to PyCWT-mod API server for hardware-accelerated computation.
"""
import requests
import numpy as np
from typing import Optional, Tuple

class RemoteWaveletBackend:
    """
    Remote computation backend using PyCWT-mod API server.
    
    This backend allows BioXen users to leverage FPGA/embedded hardware
    acceleration without local pycwt-mod installation.
    """
    
    def __init__(self, api_url: str = "http://localhost:8000"):
        self.api_url = api_url
        self._verify_connection()
    
    def _verify_connection(self):
        """Check API server availability."""
        try:
            response = requests.get(f"{self.api_url}/health", timeout=5)
            response.raise_for_status()
        except requests.exceptions.RequestException as e:
            raise ConnectionError(f"Cannot connect to PyCWT-mod server: {e}")
    
    def list_backends(self):
        """Query available hardware backends on server."""
        response = requests.get(f"{self.api_url}/api/v1/backends/")
        response.raise_for_status()
        return response.json()["backends"]
    
    def wavelet_coherence(
        self,
        signal1: np.ndarray,
        signal2: np.ndarray,
        dt: float,
        backend: str = "sequential",
        **kwargs
    ) -> Tuple[np.ndarray, np.ndarray, np.ndarray, np.ndarray, Optional[np.ndarray]]:
        """
        Compute wavelet coherence using remote server.
        
        Parameters match BioXen's local wavelet_coherence() signature.
        """
        payload = {
            "signal1": signal1.tolist(),
            "signal2": signal2.tolist(),
            "dt": dt,
            "backend": backend,
            **kwargs
        }
        
        response = requests.post(
            f"{self.api_url}/api/v1/wavelet/wct",
            json=payload,
            timeout=300  # 5 minutes for large computations
        )
        response.raise_for_status()
        
        result = response.json()
        
        # Convert back to NumPy arrays
        return (
            np.array(result["WCT"]),
            np.array(result["aWCT"]),
            np.array(result["coi"]),
            np.array(result["freqs"]),
            np.array(result["signif"]) if result.get("signif") else None
        )


# BioXen integration point
class WaveletAnalyzer:
    """
    BioXen wavelet analyzer with local and remote backend support.
    """
    
    def __init__(self, backend: str = "local", api_url: Optional[str] = None):
        """
        Initialize wavelet analyzer.
        
        Args:
            backend: 'local' for NumPy/SciPy, 'remote' for API server
            api_url: PyCWT-mod server URL (required if backend='remote')
        """
        self.backend_type = backend
        
        if backend == "remote":
            if not api_url:
                raise ValueError("api_url required for remote backend")
            self.remote = RemoteWaveletBackend(api_url)
        else:
            self.remote = None
    
    def coherence(self, signal1, signal2, dt, **kwargs):
        """
        Compute wavelet coherence.
        
        Routes to local or remote backend based on configuration.
        """
        if self.backend_type == "remote":
            return self.remote.wavelet_coherence(signal1, signal2, dt, **kwargs)
        else:
            # Local computation using NumPy/SciPy
            from bioxen.wavelet.local import compute_coherence_local
            return compute_coherence_local(signal1, signal2, dt, **kwargs)
```

**Step 2: User-Facing BioXen API**
```python
# Example: BioXen user leveraging remote FPGA acceleration
from bioxen.wavelet import WaveletAnalyzer
import numpy as np

# Generate biological signals
eeg_channel1 = np.random.randn(10000)  # 10 seconds at 1kHz
eeg_channel2 = np.random.randn(10000)

# Option 1: Local computation
analyzer_local = WaveletAnalyzer(backend='local')
wct, phase, coi, freqs, signif = analyzer_local.coherence(
    eeg_channel1, eeg_channel2, dt=0.001
)

# Option 2: Remote computation with FPGA acceleration
analyzer_remote = WaveletAnalyzer(
    backend='remote',
    api_url='http://lab-server.local:8000'
)
wct, phase, coi, freqs, signif = analyzer_remote.coherence(
    eeg_channel1, eeg_channel2, 
    dt=0.001,
    backend='elm11'  # Request Tang Nano 9K FPGA
)
print(f"âœ“ Computed with hardware acceleration on remote server")
```

---

### **Enables MVP Research (wavelets-deep-research-prompt2-mvp.md):**

**BioXen integration for automated benchmarking:**
```python
# Question 1: pycwt Performance Characterization
from bioxen.wavelet import WaveletAnalyzer
import numpy as np

analyzer = WaveletAnalyzer(backend='remote', api_url='http://localhost:8000')

# Automated performance testing across scales
for n in [100, 1000, 10000, 100000]:
    signal1 = np.sin(np.linspace(0, 10, n))
    signal2 = np.cos(np.linspace(0, 10, n))
    
    # Direct API call for benchmarking
    response = requests.post("http://localhost:8000/api/v1/benchmark", json={
        "signal_length": n,
        "mc_count": 300,
        "backends": ["sequential", "joblib", "elm11"]
    })
    
    results = response.json()['results']
    print(f"N={n}:")
    print(f"  Sequential: {results['sequential']['computation_time']:.3f}s")
    print(f"  Joblib:     {results['joblib']['computation_time']:.3f}s ({results['joblib']['speedup']:.2f}Ã—)")
    print(f"  FPGA:       {results['elm11']['computation_time']:.3f}s ({results['elm11']['speedup']:.2f}Ã—)")
```

---

### **Enables FPGA Research (FFT-based-performance-bottleneck-massive-datasets-prompt.FPGA.md):**

**Real-time streaming for BCI/adaptive stimulation:**
```python
# BioXen + WebSocket for closed-loop experiments
from bioxen.wavelet.streaming import StreamingWaveletAnalyzer
import websocket
import json

# Connect to streaming endpoint
analyzer = StreamingWaveletAnalyzer(
    ws_url='ws://localhost:8000/ws/stream/wct',
    backend='elm11'  # FPGA for <1ms latency
)

# Configure for 128-channel EEG at 1kHz
analyzer.configure(dt=0.001, window_size=256)

# Process continuous EEG stream
for chunk in eeg_data_generator():
    channel1, channel2 = chunk[0], chunk[1]
    
    # Get coherence with minimal latency
    wct_result = analyzer.process_chunk(channel1, channel2)
    
    # Latency measured on server
    print(f"Latency: {wct_result['latency_ms']:.2f}ms")
    
    # Trigger adaptive stimulation if needed
    if wct_result['coherence'] > threshold:
        trigger_stimulation()
```

---

### **Enables Comparative Analysis:**

**Multi-backend performance validation:**
```python
# BioXen wrapper for automated backend comparison
from bioxen.research.benchmark import compare_backends

# Compare all available backends (local + remote)
results = compare_backends(
    signal_length=10000,
    mc_count=300,
    backends={
        'local_numpy': {'backend': 'local'},
        'remote_cpu': {'backend': 'remote', 'remote_backend': 'sequential'},
        'remote_parallel': {'backend': 'remote', 'remote_backend': 'joblib'},
        'remote_fpga': {'backend': 'remote', 'remote_backend': 'elm11'}
    },
    api_url='http://localhost:8000'
)

for name, data in results.items():
    print(f"{name}: {data['speedup']:.2f}Ã— (latency: {data['latency_ms']:.2f}ms)")
```

---

### **Lab Environment Deployment:**

**Typical BioXen + PyCWT-mod server setup:**
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                  Research Lab Network                        â”‚
â”‚                                                              â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚  â”‚ Researcher 1 â”‚      â”‚ Researcher 2 â”‚      â”‚ Researcherâ”‚ â”‚
â”‚  â”‚ Laptop       â”‚      â”‚ Laptop       â”‚      â”‚ 3 Desktopâ”‚ â”‚
â”‚  â”‚              â”‚      â”‚              â”‚      â”‚          â”‚ â”‚
â”‚  â”‚ BioXen       â”‚      â”‚ BioXen       â”‚      â”‚ BioXen   â”‚ â”‚
â”‚  â”‚ (client)     â”‚      â”‚ (client)     â”‚      â”‚ (client) â”‚ â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜      â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜      â””â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜ â”‚
â”‚         â”‚                     â”‚                     â”‚       â”‚
â”‚         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜       â”‚
â”‚                               â”‚                             â”‚
â”‚                               â–¼                             â”‚
â”‚                  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                â”‚
â”‚                  â”‚  Lab Server            â”‚                â”‚
â”‚                  â”‚  lab-server.local:8000 â”‚                â”‚
â”‚                  â”‚                        â”‚                â”‚
â”‚                  â”‚  PyCWT-mod API         â”‚                â”‚
â”‚                  â”‚  + Tang Nano 9K FPGA   â”‚                â”‚
â”‚                  â”‚  + Redis queue         â”‚                â”‚
â”‚                  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                â”‚
â”‚                                                              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Configuration:
1. Server runs: docker-compose up -d
2. Clients use: WaveletAnalyzer(backend='remote', api_url='http://lab-server.local:8000')
3. Shared FPGA resource, multiple users benefit
```

---

## âš ï¸ Risk Mitigation

### **Technical Risks:**

| Risk | Impact | Mitigation |
|------|--------|------------|
| Hardware detection fails | High | Graceful fallback, clear error messages |
| FPGA driver issues | Medium | Comprehensive detection script, troubleshooting docs |
| NumPy serialization issues | Medium | Custom JSON encoders for complex/numpy types |
| Async overhead | Low | Benchmark against direct pycwt usage |

### **Operational Risks:**

| Risk | Impact | Mitigation |
|------|--------|------------|
| Concurrent job overload | High | Redis queue with max workers limit |
| Memory exhaustion | High | Input validation (max array size) |
| WebSocket connection leaks | Medium | Connection timeout, cleanup on disconnect |

---

## ðŸ“š Documentation Deliverables

1. **API Reference** - Auto-generated from FastAPI (Swagger UI)
2. **Integration Guide** - How to use from Python/JS/R/MATLAB
3. **Deployment Guide** - Docker, systemd, production config
4. **Performance Guide** - Backend selection, optimization tips
5. **Troubleshooting Guide** - Common issues and solutions

---

## ðŸš€ Future Enhancements (Post v1.0)

1. **Authentication & Authorization**
   - API key management
   - JWT tokens for user sessions
   - Rate limiting per user

2. **Advanced Features**
   - Multi-dimensional array support (3D wavelet transforms)
   - Custom wavelet definitions via API
   - Result caching for repeated queries

3. **Monitoring & Analytics**
   - Prometheus metrics endpoint
   - Grafana dashboard
   - Usage analytics

4. **Additional Backends**
   - GPU acceleration (CUDA)
   - Distributed processing (Dask clusters)
   - Cloud backend (AWS Lambda)

---

## ðŸŽ“ Conclusion

This implementation plan provides a complete roadmap from the API specification to a production-ready REST API server that **extends BioXen_Fourier_lib** with remote hardware-accelerated wavelet analysis capabilities.

### **Key Architectural Benefits:**

1. **BioXen Extension Architecture:**
   - âœ… BioXen users access FPGA acceleration without local hardware
   - âœ… Transparent backend switching (local â†” remote)
   - âœ… Shared lab resources (one FPGA server, multiple clients)
   - âœ… Consistent API across local and remote computation

2. **Technical Capabilities:**
   - âœ… Exposes all pycwt-mod backends via HTTP/WebSocket
   - âœ… Supports hardware acceleration (FPGA, embedded, GPU future)
   - âœ… Enables research validation (MVP, FPGA, Phase 2)
   - âœ… Real-time streaming for closed-loop experiments
   - âœ… Automated benchmarking and performance comparison

3. **Cross-Platform Integration:**
   - âœ… Python client library (BioXen integration)
   - âœ… JavaScript/Node.js client (web applications)
   - âœ… R and MATLAB integration (research workflows)
   - âœ… Direct HTTP/cURL access (any language)

4. **Production Readiness:**
   - âœ… Comprehensive testing and documentation
   - âœ… Docker deployment with device passthrough
   - âœ… Job queue for batch processing
   - âœ… Health monitoring and error handling

### **Deployment Models:**

**Model 1: Individual Researcher**
- Local BioXen + local API server on same machine
- Use case: Personal development, testing

**Model 2: Lab Server**
- Multiple BioXen clients â†’ shared API server with FPGA
- Use case: Research lab with centralized hardware

**Model 3: Cloud Deployment**
- BioXen clients â†’ cloud API server â†’ distributed workers
- Use case: High-throughput analysis, external collaborators

### **Project Timeline:**

| Phase | Duration | Deliverable |
|-------|----------|-------------|
| Phase 1-2 | 2 weeks | Core API + Backend management |
| Phase 3 | 1 week | Wavelet endpoints |
| Phase 4 | 1 week | Real-time streaming |
| Phase 5 | 1 week | Hardware detection + benchmarking |
| Phase 6 | 1 week | Job queue |
| Phase 7 | 1 week | Testing + docs |
| Phase 8 | 1 week | Deployment |
| **Total** | **8 weeks** | **Production API server** |

**Estimated Total Effort:** 6-8 weeks for single developer  
**Priority:** High (enables BioXen hardware acceleration + research validation)  
**Next Step:** Phase 1 implementation (Core API Infrastructure)

---

## ðŸ“¦ Deliverables Summary

### **For BioXen_Fourier_lib Users:**
- Remote backend class (`bioxen.wavelet.remote_backend.py`)
- Transparent local/remote switching
- Example notebooks demonstrating FPGA acceleration

### **For API Server Deployment:**
- Complete FastAPI application
- Docker configuration with FPGA passthrough
- Deployment guides (Docker, systemd, cloud)

### **For Researchers:**
- Automated benchmark endpoints (validates MVP research)
- Real-time streaming (validates FPGA latency research)
- Performance comparison tools (validates comparative analysis)

### **For External Integration:**
- OpenAPI/Swagger documentation
- Client libraries (Python, JavaScript, R, MATLAB)
- Integration examples and tutorials

**Repository:** `BioXen_Fourier_lib/` (contains both library and API server)  
**Server Location:** `BioXen_Fourier_lib/server/` (new directory)  
**Client Integration:** `BioXen_Fourier_lib/bioxen/wavelet/remote_backend.py` (new file)
